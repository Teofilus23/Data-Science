{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 Mengenal Text Processing: Bag of Words & Stop Word Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- NRP : 1973005\n",
    "- Nama : Ferly Jeremi Purnawna Apiang\n",
    "\n",
    "mempelajari 2 macam text processing yang umum ditemui dalam bidang machine learning yaitu :\n",
    "\n",
    "- Bag of Words.\n",
    "- Stop Word Filtering.\n",
    "\n",
    "Dari pembelajaran sebelumnya, komputer atau machine tidak bisa memahami data text dengan baik. Lalu bagimana ketika dihadapkan pada sekumpulan data text dalam bentuk kalimat atau dokumen? Dalam machine learning, terdapat bidang yang secara spesifik membahas dataset text, bidang ini dikenal sebagai \"Natural Language Processing\" atau yang biasanya disingkat sebagai NLP. Yang kita pelajari pada sesi kali ini bisa kita manfaatkan sebagai pengantar NLP.\n",
    "\n",
    "Dalam bidang machine learning sendiri terdapat beberapa teknik yang umum digunakan untuk melakukan features extraction dari dataset text. Pada sesi pemebelajaran kali ini, kita akan mempelajari dua diantaranya, yaitu \"Bag of Words\" dan \"Stop Word Filtering\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words model sebagai representasi text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of Words menyederhanakan representasi text sebagai sekumpulan kata serta mengabaikan grammar dan posisi tiap kata pada kalimat. Text akan dikonversi menjadi lowercase dan tanda baca akan diabaikan.\n",
    "\n",
    "Referensi: https://en.wikipedia.org/wiki/Bag-of-words_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Linux has been around since the mid-1990s.',\n",
       " 'Linux distributions include the Linux kernel.',\n",
       " 'Linux is one of the most prominent open-source software.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [\n",
    "    'Linux has been around since the mid-1990s.',\n",
    "    'Linux distributions include the Linux kernel.',\n",
    "    'Linux is one of the most prominent open-source software.'\n",
    "]\n",
    "\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Siapkan terlebih dahulu suatu data text. Untuk kasus kita kali ini, dataset nya berupa sekumpulan kalimat pendek atau string. Dataset text ini juga seringkali dikenal dengan istilah \"Corpus\".\n",
    "- Untuk kasus kita kali ini, corpus kita terdiri dari tiga kalimat pendek yaitu : kalimat pertama adalah 'Linux has been around since the mid-1990s.'. Kalimat kedua 'Linux distribuutions include the linux kernel.', dan kalimat ketiga adalah 'Linux is one of the most prominent open-source software.'.\n",
    "- Ketiga kalimat tersebut akan kita tampung kedalam suatu list yang kemudian kita assign kedalam suatu variabel yang kita beri nama 'corpus'.\n",
    "- Lalu terakhir kita akan coba tampilkan isi dari variabel 'corpus' tersebut.\n",
    "\n",
    "Dataset text ini sering disebut corpus. Corpus terdiri dari 3 kalimat pendek. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words model dengan CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of Words model dapat diterapkan dengan memanfatkan CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1],\n",
       "        [0, 0, 0, 1, 0, 1, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1]],\n",
       "       dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorized_X = vectorizer.fit_transform(corpus).todense()\n",
    "vectorized_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- import terlebih dahulu modulnya dengan memanggil \"from sklearn.feature_extraction.text import CountVectorizer\".\n",
    "- Lalu selanjutnya kita akan membentuk objek dari kelas \"CountVectorizer\" dengan memanggil \"CountVectorizer()\" yang akan ditampung kedalam variabel \"vectorizer\".\n",
    "- Selanjutnya kita akan menggunakan objek \"vectorizer\" ini untuk menerapkan method fit transform terhadap \"corpus\" dataset lalu hasil yang terbentuk akan kita konversikan kedalam suatu array. Oleh karenanya, disini kita memanggil method \"todense()\" atau dengan kata lain method todense ini akan mengkonversikan hasil fit transform dari objek \"vectorizer\" tersebut menjadi suatu array dua dimensi yang diamana objek array dua dimensi terebut akan ditampung kedalam variabel \"vectorized_X\".\n",
    "\n",
    "Bag of words model dapat di terapkan dengan CountVectorizer. variabel vectorizer menampung hasil objek. Vectorizer.fit_transform(corpus).todense() menerapkan method fit transform terhadap corpus dataset, todense() berfungsi untuk mengubah hasil fit tranform menjadi array dua dimensi. Dan Variabel vectorized_X menampung hasil array\n",
    " \n",
    "hasilnya berupa array 2 dimensi yang setiap barisnya akan merepresentasikan tiap kalimat yang berada dalam corpus yang dimiliki.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1990s',\n",
       " 'around',\n",
       " 'been',\n",
       " 'distributions',\n",
       " 'has',\n",
       " 'include',\n",
       " 'is',\n",
       " 'kernel',\n",
       " 'linux',\n",
       " 'mid',\n",
       " 'most',\n",
       " 'of',\n",
       " 'one',\n",
       " 'open',\n",
       " 'prominent',\n",
       " 'since',\n",
       " 'software',\n",
       " 'source',\n",
       " 'the']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kumpulan kata diurutkan berdasarkan Alphabetical Order, semua case menjadi lowercase dan setiap nilai merepresentasikan jumlah kemunculan kata tertentu pada kalimat.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euclidean Distance untuk mengukur kedekatan/jarak antar dokumen (vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jarak dokumen 1 dan 2: [[3.16227766]]\n",
      "Jarak dokumen 1 dan 3: [[3.74165739]]\n",
      "Jarak dokumen 2 dan 3: [[3.46410162]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "for i in range(len(vectorized_X)):\n",
    "    for j in range(i, len(vectorized_X)):\n",
    "        if i == j:\n",
    "            continue\n",
    "        jarak = euclidean_distances(vectorized_X[i], vectorized_X[j])\n",
    "        print(f'Jarak dokumen {i+1} dan {j+1}: {jarak}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dengan representasi Bag Of Words suatu algoritma machine learning dapat dengan lebih mudah mengukur kedekatan atau kemiripan antar dokumen. Euclidean distances ini untuk mengukur kedekatan.\n",
    "\n",
    "jarak antara dokumen pertama dan kedua adalah 3.16227766, lalu jarak antara dokumen pertama dengan dokumen ketiga adalah 3.74165739, dan jarak antara dokumen kedua dengan dokumen ketiga adalah 3.46410162. Jika kita perhatikan disini, nilai terkecilnya adalah jarak antar dokumen pertama dan kedua atau dengan kata lain disini kita bisa simpulkan bahwa tingkat kemiripan antara dokumen pertama dengan dokumen kedua adalah yang tertinggi diantara ketiga dokumen tersebut."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Word Filtering pada text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop Word Filtering menyederhanakan representasi text dengan mengabaikan beberapa kata seperti determiners (the, a, an), auxiliary verbs (do, be, will), dan prepositions (on, in, at).\n",
    "\n",
    "Referensi: https://en.wikipedia.org/wiki/Stop_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Linux has been around since the mid-1990s.',\n",
       " 'Linux distributions include the Linux kernel.',\n",
       " 'Linux is one of the most prominent open-source software.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "terdapat beberapa stop word yaitu \"the\", \"has\", \"been\", \"is\", dan \"of\" yang akan di stop word atau di abaikan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Word Filtering dengan CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop Word Filtering juga dapat diterapkan dengan memanfatkan CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 0, 0, 0, 1, 1, 0, 0, 0, 0],\n",
       "        [0, 1, 1, 1, 2, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 1, 0, 1, 1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "vectorized_X = vectorizer.fit_transform(corpus).todense()\n",
    "\n",
    "vectorized_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mengimport terlebih dahulu modulnya dengan memanggil \"from sklearn.feature_extraction.text import CountVectorizer\".\n",
    "- Lalu selanjutnya kita akan membentuk terlebih dahulu objek dari class Count Vectorizer tersebut. Yang berbeda kali ini adalah sewaktu kita membentuk objek Count Vectorizer, kita juga akan menyertakan parameter \"stop_words\" yang kita beri nilai 'english' karena pada kasus kali ini untuk melakukan stop word filtering untuk bahasa inggris. Lalu selanjutnya objek Count Vectorizer yang terbentuk akan kita tampung kedalam variabel \"vectorized\".\n",
    "- Objek \"vectorized\" ini kemudian akan kita panggil untuk menerapkan \"fit_tranform\" terhadap corpus yang kita miliki dan hasil tranformnya kita akan konversikan ke dalam array dua dimensi. Oleh karenanya, kita akan memnaggil method \".todense()\" dan array dua dimensi yang terbentuk akan kita tampung kedalam variabel \"vectorized_X\".\n",
    "- Lalu kita tampilkan hasilnya.\n",
    "\n",
    "\n",
    "Stop Word Filtering untuk mengeluarkan Stop Word dari corpus. Lalu membentuk objek dari class CountVectorizer(stop_words='english'). Pada variabel vectorized_X menampung hasil objek dan vectorizer.fit_transform(corpus).todense() menerapkan method fit transform terhadap corpus dataset, todense() berfungsi untuk mengubah hasil fit tranform menjadi array dua dimensi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1990s',\n",
       " 'distributions',\n",
       " 'include',\n",
       " 'kernel',\n",
       " 'linux',\n",
       " 'mid',\n",
       " 'open',\n",
       " 'prominent',\n",
       " 'software',\n",
       " 'source']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setelah melakukan Stop Word Filtering alhasil memperoleh representasi suatu kalimat yang lebih sederhana"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
